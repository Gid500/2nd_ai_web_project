{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea958a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91857e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 957 images belonging to 6 classes.\n",
      "Found 104 images belonging to 6 classes.\n",
      "Starting model training...\n",
      "Epoch 1/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 579ms/step - accuracy: 0.6583 - loss: 1.2405 - val_accuracy: 0.7981 - val_loss: 0.7677\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 545ms/step - accuracy: 0.7471 - loss: 0.7875 - val_accuracy: 0.7885 - val_loss: 0.7886\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 549ms/step - accuracy: 0.7722 - loss: 0.7256 - val_accuracy: 0.8173 - val_loss: 0.6998\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 542ms/step - accuracy: 0.7858 - loss: 0.6608 - val_accuracy: 0.8077 - val_loss: 0.6984\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 541ms/step - accuracy: 0.7962 - loss: 0.6232 - val_accuracy: 0.8462 - val_loss: 0.6810\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 540ms/step - accuracy: 0.8046 - loss: 0.5821 - val_accuracy: 0.8173 - val_loss: 0.6691\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 540ms/step - accuracy: 0.8130 - loss: 0.5306 - val_accuracy: 0.8173 - val_loss: 0.7554\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 547ms/step - accuracy: 0.8391 - loss: 0.4992 - val_accuracy: 0.8269 - val_loss: 0.6454\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 549ms/step - accuracy: 0.8182 - loss: 0.5216 - val_accuracy: 0.8173 - val_loss: 0.5797\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 539ms/step - accuracy: 0.8297 - loss: 0.4800 - val_accuracy: 0.8462 - val_loss: 0.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training finished.\n",
      "Model saved to /home/qod120/Documents/project/2nd_ai_web_project/ai_model/cat_emotion_mobilenet_v2.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # You might want to increase this for better accuracy\n",
    "NUM_CLASSES = 6 # angry, Happy, Normal, relaxed, sad, Scared\n",
    "DATASET_PATH = \"/home/qod120/Documents/project/2nd_ai_web_project/ai_model/Cat Emotions.v2i.folder\"\n",
    "TRAIN_DIR = os.path.join(DATASET_PATH, \"train\")\n",
    "\n",
    "MODEL_SAVE_PATH = \"/home/qod120/Documents/project/2nd_ai_web_project/ai_model/cat_emotion_mobilenet_v2.h5\"\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.1 # 10% for validation\n",
    ")\n",
    "\n",
    "# Create the generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # For one-hot encoded labels\n",
    "    subset='training', # Set as training data\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # For one-hot encoded labels\n",
    "    subset='validation', # Set as validation data\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load MobileNetV2 pre-trained on ImageNet\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Add custom top layers for single-label classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x) # Added a new Dense layer\n",
    "x = Dropout(0.5)(x) # Added a Dropout layer for regularization\n",
    "x = Dense(NUM_CLASSES, activation='softmax')(x) # Softmax for single-label classification\n",
    "\n",
    "# Create the new model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', # Use categorical_crossentropy for single-label classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=math.ceil(train_generator.samples / BATCH_SIZE),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=math.ceil(validation_generator.samples / BATCH_SIZE),\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
